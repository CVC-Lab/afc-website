{"componentChunkName":"component---src-pages-markdown-remark-frontmatter-slug-js","path":"/camera-isp/","result":{"data":{"markdownRemark":{"html":"<h1 id=\"camera-isp-code--paper\">Camera ISP Code &#x26; Paper</h1>\n<p>The realm of Camera Image Signal Processing (ISP) entails the development of\nadvanced algorithms and techniques to process and enhance the quality of images\ncaptured by camera sensors. Our contributions to this field have yielded a\ncollection of methodologies and approaches that cater to different aspects of\ncamera ISP. From denoising to scene synthesis, our work has paved the way for\nadvancements in camera technology, providing robust, adaptable, and innovative\nmethods to address current challenges. The implications of these contributions\nhold promising potential for future developments in camera imaging, sensor\ntechnologies, and multimedia applications. We outline these contributions\nbelow:</p>\n<h2 id=\"deep-contrastive-patch-based-subspace-learning-for-camera-image-signal-processing\">Deep Contrastive Patch-Based Subspace Learning for Camera Image Signal Processing</h2>\n<p>Our work on Deep Contrastive Patch-Based Subspace Learning (Yang et al., 2021)\nled to the creation of the Patch Subspace Learning Autoencoder (PSL-AE). This\ndeep neural network model employs a patch-based, local subspace approach that\nemphasizes robust heterogeneous artifacts, focusing specifically on image\ndenoising. The ability to process and eliminate noise at a local subspace level\nallows for greater precision and effectiveness in enhancing the overall image\nquality.</p>\n<h2 id=\"reinforcement-learning-of-self-enhancing-camera-image-and-signal-processing\">Reinforcement Learning of Self Enhancing Camera Image and Signal Processing</h2>\n<p>The Recursive Self Enhancement Reinforcement Learning (RSE-RL) model\n(Bajaj et al., 2023) was introduced to employ deep reinforcement\nlearning for spatially adaptive artifact filtering. This approach has\ndemonstrated significant advantages in heterogeneous noise and artifact\nremoval. By using reinforcement learning to adapt to the spatial\ncharacteristics of the image, this method provides a dynamic solution to the\nchallenges of noise and artifact mitigation.</p>\n<figure id='RSE-RL'>\n<span class='gatsby-resp-image-wrapper' style='position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; '>\n      <a class='gatsby-resp-image-link' href='/static/9e42a2dc253a694609f9283a1dd78ab7/ad12c/RSE-RL.png' style='display: block' target='_blank' rel='noopener'>\n    <span class='gatsby-resp-image-background-image' style=\"padding-bottom: 56.49999999999999%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAABYlAAAWJQFJUiTwAAACLUlEQVR42m1T227UMBDNTyPEX/AK4oGLxCOCF7SoVReKkIqgFFqI9pruZpNsTDZOYufm3A5jhxV9YCTLsT1z5syZiQWyrutQ1zWatkHT/Ftd36OqKsSHCFEUIU1TtG07vlFMWeb0XqJWypy1WVIIJEmMns4FBTRKUsD4WIsU8T6EzyQ6Stb3g7lv2x4hC2B/eYIk9XEkZQDPJ2+xnt2gFMDsdIpo/RJJVqHtBtS5xC/nBw6ZGIM047qCkDk4j+EuJ/D2W4hcISVSBtC+/ArvdoHQqzCbfsTu8z28fneKy3kMwTNkKYcsh78serCQQeYVfN+Bc/MC3m+G6+s5Prx5NQJeXVzA26ygCsA+IYaLp9h4DFlRkxQFgiCFy7akr4LqSE8ZIc8LAi2wW53ADxyoZoBS9QioRa/rEn1LGsYJ7TmOVlYK0veRyxgs8eHGt4hETP4KLDpgfvWcyg8RbD08e/QYPOawNBPbjRGnAmlGTWlaI7DWaxio1H4Ue6CGlEVpyjb3dxrx6WyKh/cfQCQJrDAROLc32Pp7zGczuK4Lx3EgxNiIY7C2oiyIXQUpJYqiMG965zwBo2nQ42Rx/4Cfk2/Yu4EB0mu5XJLovpnBu6bnkHOOMAxxS34ey2HPFpRAmFnUM2rtDwHefz/DLthhtVphsVwY0CAIzDD3VPqRaZZlhrm+15VkNC5rZ2MYaylMU8qqBeOKnKXJzBgzZfzPNLgiJvqvOibSvsdvDfgHNLxAjPqqSSUAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class='gatsby-resp-image-image' alt='Diagram illustrating the RSE-RL pipeline' title='' src='/static/9e42a2dc253a694609f9283a1dd78ab7/5a190/RSE-RL.png' srcset='/static/9e42a2dc253a694609f9283a1dd78ab7/772e8/RSE-RL.png 200w,\n/static/9e42a2dc253a694609f9283a1dd78ab7/e17e5/RSE-RL.png 400w,\n/static/9e42a2dc253a694609f9283a1dd78ab7/5a190/RSE-RL.png 800w,\n/static/9e42a2dc253a694609f9283a1dd78ab7/ad12c/RSE-RL.png 856w' sizes='(max-width: 800px) 100vw, 800px' style='width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;' loading='lazy' decoding='async'>\n  </a>\n    </span>\n<figcaption>Figure 1: The overall pipeline of our RSE-RL</figcaption>\n</figure>\n<p>For each captured (and training)\nimage, we split the image into local patches and feed every patch as a stack\ninto the encoding network. We divide the latent space into three subspaces\n<em>Z</em><sub><em>y</em></sub>, <em>Z</em><sub><em>u</em></sub>, and <em>Z</em><sub><em>v</em></sub> which preserve the\nYUV features of the patches. We apply the encoder to the clean and noisy\npatches and project them as well onto the same three latent subspaces. Then, we\nlearn a set of transformations T that transforms the latent representation of\nthe noisy patches to a corresponding representation of the clean patches, in\nall three subspaces. We send individually sampled transformed noisy\nrepresentations to the decoders to reconstruct the image. Note that we have\nthree decoders to reconstruct YUV features from the three subspaces. Then we\nmerge and reconvert from the YUV features to reconstruct the final RGB image.\nOnce we reconstruct the images, we train a soft-actor-critic reinforcement\nlearning algorithm to further maximize the image PSNR. The RL algorithm uses a\n1-norm distance between a target PSNR and the actual PSNR as the reward to\nfine-tune the trainable weights in the set of transformations <em>T</em>.</p>\n<h2 id=\"invariance-based-multi-clustering-of-latent-space-embeddings-for-equivariant-learning\">Invariance-based Multi-clustering of Latent Space Embeddings for Equivariant Learning</h2>\n<p>In the domain of image recognition, our work on Invariance-based\nMulti-clustering of Latent Space Embeddings (Bajaj et al., 2021) uses\nVariational Autoencoders (VAEs) to learn invariant and equivariant clusters in\nlatent space. The novel separation of semantic and equivariant variables\nprovides enhanced capabilities for image recognition, offering a structured and\ntargeted approach to discerning and interpreting image content.</p>\n<h2 id=\"scene-synthesis-via-uncertainty-driven-attribute-synchronization\">Scene Synthesis via Uncertainty-Driven Attribute Synchronization</h2>\n<p>Finally, our work on Scene Synthesis via Uncertainty-Driven Attribute\nSynchronization (Yang et al., 2021) offers a novel approach to 3D scene\nsynthesis. Integrating neural network-based and conventional methods, this\napproach captures diverse 3D scene patterns, outperforming existing\nmethodologies. Its relevance to Camera ISP lies in the ability to synthesize\nand model complex scenes, offering new possibilities for camera imaging\napplications.</p>\n<h4 id=\"references\">References</h4>\n<ul>\n<li>\n<p>Bajaj, C., Roy, A., &#x26; Zhang, H. (2021). Invariance-based multi-clustering of\nlatent space embeddings for equivariant learning. ArXiv Preprint ArXiv:\n2107.11717.</p>\n</li>\n<li>\n<p>Bajaj, C., Yang, Y., &#x26; Wang, Y. (2023). Reinforcement Learning of\nSelf-enhancing Camera Image and Signal Processing. In Advances in Data-driven\nComputing and Intelligent Systems: Selected Papers from ADCIS 2022, Volume\n2 (pp. 281â€“303). Springer.</p>\n</li>\n<li>\n<p>Yang, Y., Zheng, Y., Wang, Y., &#x26; Bajaj, C. (2021). Deep Contrastive\nPatch-Based Subspace Learning for Camera Image Signal Processing. <em>arXiv\npreprint arXiv:2104.00253</em>.</p>\n</li>\n<li>\n<p>Yang, H., Zhang, Z., Yan, S., Huang, H., Ma, C., Zheng, Y., Bajaj, C., &#x26;\nHuang, Q. (2021). Scene synthesis via uncertainty-driven attribute\nsynchronization.</p>\n</li>\n</ul>","frontmatter":{"title":"Camera ISP Code & Paper"}}},"pageContext":{"id":"2fe80898-b5b7-52cc-bc52-97eadab44f89","frontmatter__slug":"/camera-isp","__params":{"frontmatter__slug":"camera-isp"}}},"staticQueryHashes":["429448491"],"slicesMap":{}}