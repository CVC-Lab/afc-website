{"componentChunkName":"component---src-pages-markdown-remark-frontmatter-slug-js","path":"/signature-detection/","result":{"data":{"markdownRemark":{"html":"<h1 id=\"signature-detection--classification\">Signature Detection &#x26; Classification</h1>\n<p>Signature Detection and Classification centers on identifying and categorizing\nspecific features\nwithin data. Our research has yielded two major contributions: the\napplication of neuromorphic principles and unsupervised hyperspectral\nimage segmentation, addressing challenges from deblurring to image\nsegmentation. These advancements not only push the boundaries of\nSignature Detection but extend to various data analysis domains. In the\nfield of Integration of IR/EO Video Streams, we've laid milestones in\ncomputer vision through methods like consistent shape matching and image\nintensity recovery. By synthesizing data-driven modeling, parametric\nmesh generation, and event-enhanced algorithms, we've created building\nblocks for future video processing systems. These contributions are\noutlined as follows:</p>\n<h2 id=\"deblursr-event-based-motion-deblurring-under-the-spiking-representation\">DeblurSR: Event-Based Motion Deblurring Under the Spiking Representation</h2>\n<p>DeblurSR presents\na novel approach to motion deblurring, converting a blurry image into a\nsharp video (Song et al., 2023). Leveraging event data and the spiking\nrepresentation (SR), DeblurSR compensates for motion ambiguities and\nparameterizes the output video as a time-to-intensity mapping. Inspired\nby the biological principles governing neuron communication in living\norganisms, the SR explains the representation of sharp edges and\ninterprets spiking parameters from a neuromorphic perspective.</p>\n<p>The advantages of DeblurSR include superior output quality and reduced\ncomputational resources compared to contemporary event-based motion\ndeblurring methods. Notably, our approach is easily extendable to video\nsuper-resolution, particularly when integrated with the latest\ndevelopments in implicit neural representation. This contribution to\nSignature Detection accentuates the fusion of neuromorphic principles\nwith imaging technology, forging new pathways in image deblurring. The\narchitecture is shown in Figure 1.</p>\n<figure id='DeblurSRPipeline'>\n<span class='gatsby-resp-image-wrapper' style='position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; '>\n      <a class='gatsby-resp-image-link' href='/afc-website/static/10f709817697256e7c41a2d6130e88b8/9eaa0/DeblurSR.png' style='display: block' target='_blank' rel='noopener'>\n    <span class='gatsby-resp-image-background-image' style=\"padding-bottom: 70%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAABYlAAAWJQFJUiTwAAADHklEQVR42o2Ty0srVxzH50/ysWlsFdy4UFTwgU9UvEqhKgXtShDplfZWEJfddSd02XqlVlETKr0NCpf6iK9Y837HTG4mMyYzmcyn50xaSjelB4Zz5vxe39/39z2KZVnE43EikQjBYJBCoYADWJZJTdik3a7X+b9LMQyDWCxGOBwmk8mgqkXkXVIkTheLpFUVvaz95e78K9hxnMaN849JMU0TTdPI5/MYuk7BqFC6uebl4gjNMTD8b3kJR9AF0ogoHBTfH9EoqiioC3+9XBa7RrlcQuZSSqUSe3t7nJ+f83b3R8o1G+Pygur3X6A+/oD+3efY8TT5qsXvVzeceU+4fP+eRCbLw+Mjj9EUoWCO8FMOTTdQJH8dHR0sLi7ysceD/+IKM/RE9dJHHRvz/GfsRBJDtBd6CBL/zU/08oq8QCi7SiRTRGMJMYdEI2E2m2V2dpbt7W13Pzo+cbkwazUswYtt1yVZ7mfbNjVhq4lzXZzrYliOU8fSXyg8C3/RnaIK0re2tvhpf99NKqv+vSoVjedCGktwI6etGzpl9QNGscRLtUJNr1BJP/N0dkss0VCDksvl6Gj/hPm5V3g8H3Hk+6WB0KqJ1ncwH75F1YpUbIcPgu9oKMyNaDkpFJFL5skH4wQCUSJJDb1qSoRFXn36GV99s8XExAQHBwduQonIqYu2bMttVS6phvv7e359947buztso4oukgbP7oglTVc7ipRPxaqTzqvI9v9rSd3JQma16tJQl/9mlWIiSzZjNXSYSqW4v7vlUCC7vr52X43P5+P09BRp8/v9HB4eusIPBALs7u66KOVj2Be8Hx0f8xQK4T3x4vV5UXZ2dpiammZubo43b75mZWWF6elp1tbW2NjYYHR0lKWlJTY3N4XfFOPj4+7wFhYWmJ+fZ319nddfvnZzrK6uovT29tLU1ERfXx89PT00NzfT1dXFyMgILS0ttLW1MTg4SHt7O62tra5fd3e3GyP3gYEB994jNLy8vIzS39/P0NAQnZ2dbqKxsTEXlQyUu0Qkg2SByclJF6X0kQOUnQwPD7txEszMzAx/Atvdmyn5p8QYAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class='gatsby-resp-image-image' alt='Flowchart showing process of deblurring an image' title='' src='/afc-website/static/10f709817697256e7c41a2d6130e88b8/5a190/DeblurSR.png' srcset='/afc-website/static/10f709817697256e7c41a2d6130e88b8/772e8/DeblurSR.png 200w,\n/afc-website/static/10f709817697256e7c41a2d6130e88b8/e17e5/DeblurSR.png 400w,\n/afc-website/static/10f709817697256e7c41a2d6130e88b8/5a190/DeblurSR.png 800w,\n/afc-website/static/10f709817697256e7c41a2d6130e88b8/c1b63/DeblurSR.png 1200w,\n/afc-website/static/10f709817697256e7c41a2d6130e88b8/29007/DeblurSR.png 1600w,\n/afc-website/static/10f709817697256e7c41a2d6130e88b8/9eaa0/DeblurSR.png 1676w' sizes='(max-width: 800px) 100vw, 800px' style='width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;' loading='lazy' decoding='async'>\n  </a>\n    </span>\n<figcaption>Figure 1: DeblurSR Pipeline.</figcaption>\n</figure>\n<p>Given a blurry image and its associated events in the exposure interval,\nwe apply a Convolutional Neural Network (CNN) to extract an image\nembedding with the same spatial resolution as the input. For each pixel\n(<em>x</em>,<em>y</em>), we fuse the image embedding with the coordinate embedding\nusing the addition operation. A group of fully-connected layers take the\nresulting per-pixel feature vector as input and regress the spiking\nparameters for each pixel as output. At time <em>t</em><sub><em>r</em></sub>, we\nassemble a spatially varying kernel from the predicted spiking\nparameters. The convolution of this kernel with the input blurry image\ngives the output sharp frame at time <em>t</em><sub><em>r</em></sub>. By changing the\ntimestamps, the spiking representation allows DeblurSR to render a sharp\nvideo with an arbitrarily high frame-rate.</p>\n<p><strong>Code:</strong> The code for this project is available on the SharePoint. We\nexperimented DeblurSR on the REDS dataset and the HDF dataset. Please\nrefer to REDS_Dataset.md and HQF_Dataset.md for instructions on how to\nset up these two datasets.</p>\n<h2 id=\"a-distribution-dependent-mumford-shah-model-for-unsupervised-hyperspectral-image-segmentation\">A Distribution-dependent Mumford-Shah Model for Unsupervised Hyperspectral Image Segmentation</h2>\n<p>The challenge of unsupervised hyperspectral image segmentation is addressed in\nour work on a\ndistribution-dependent Mumford-Shah (MS) model (Cohrs et al., 2022).\nHyperspectral images, encapsulating detailed spectral data for each\npixel, demand intricate segmentation into different classes, a task\ncompounded by spectral variability and noise.</p>\n<p>Our framework commences with denoising and dimensionality reduction\nusing the well-established Minimum Noise Fraction (MNF) transform,\nfollowed by the application of the MS segmentation functional. Enhanced\nwith a robust distribution-dependent indicator function, the MS\nfunctional is tailored to the unique challenges of hyperspectral data.\nAn efficient fixed-point iteration scheme optimizes the objective\nfunction, leading to competitive results that substantially outperform\nseveral state-of-the-art methods on benchmark datasets.</p>\n<p>This contribution to Signature Detection highlights the integration of\nunsupervised learning with hyperspectral imaging, providing a nuanced\napproach to classifying complex spectral data. Figure 2 compares segmentation\nmethods on the Pavia\nUniversity dataset, including the ground truth, k-means, GMM, BGM,\n3D-CAE, MS-2, and our proposed method. The Pavia University dataset\npresented unique challenges, and the figure illustrates the comparative\nperformance of various approaches, demonstrating the effectiveness of\nour method.</p>\n<figure id='HyperspectralImageSegmentationComparison'>\n<span class='gatsby-resp-image-wrapper' style='position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; '>\n      <a class='gatsby-resp-image-link' href='/afc-website/static/ce6fc190517636fbeb62231ae57ae8b8/ea964/msiplib.png' style='display: block' target='_blank' rel='noopener'>\n    <span class='gatsby-resp-image-background-image' style=\"padding-bottom: 90.49999999999999%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAYAAABb0P4QAAAACXBIWXMAABYlAAAWJQFJUiTwAAAEcklEQVR42m2T+0+TVxjHX1p6v18obSkFhFEqTkDwCkJLa1vbCiKXgWwOb8Ayxli8Zps6sy2LTtTFuFu2OU10im6Zk2UzFDdR0LhMl8WYXbP9tD8CL58dXtyyxL3JSc755Hm/53m+z3mkaH0dxXOrKQwEsFostLe30935BKXzF1ISDGIyGmlra6O7oxV/cZA5gVKMRgPr1q2jeVVKsDKKBLNYzPK/UvfyAE6tAoXegSRJNIRC7N/1HD63A6XeLrNQuJGh1hB6sc82OWWWTqfpbqlHo5JQ6W0yC4s4KZyOEyxVofN5ZgUbwnT19hAsz8Hk8T0MDJFq6SF/jh1z0SxLppKEVrRS6FNh9HtlFmkMI81dUENeoYmlywKotCbql9fRkEiQW5JDsKwYZbZGZvWxZnQ5fgLVQSSFmkQiRk1dFJ3RTLAqgCJbK6prQKoorSRLb+bDVzawqKqcJaEIq9c2IWlzGXgqxtxCN8vCcVakhT9ZLoYGNlCSbyOabqE2mhYsl8GuZirL8lkaiiHNL56HVmRmc874pSTe3ku8NYXG5cBim/Um2bmJULJLXOLGbHPJrPXpQRY2iIv1Hozm2X9XdvQiLXo8iNlnw+P3oZtpQHQlqVQCg8dEbqAESakgHI2xMpESeytWX6koWVySbhZ+iyZkC+YXdunVwusIUiSyFEO+m94KHR0eibr6RrqebEPjdtIcq2ZVkURtbR3ta0TJKisb2mvYtb6ecCRKclWTzPoHmnj1+TSLl9Qi3ZnKMPb5aW6PHuPW6AmuTV7h55vXBRth8spXTIyeYmriMr/dmmT8wojYn+fOj1f57sZ1bv8wxdgXI1z7PsNPt69xfWoK6d79+8x8F8cyxFd30rWxn3giTvPqFoYPHqSlaz1d6/tIJJM0CXb48FukRFxnTx9JweLiRdy8eUvWuC+0pLt3p+XDpYsXqAjko9RYROeyxFLw+p6drIlUkaUyC9Oz5DX8xh7qFohnIscp5AZlMhlZY3p6WmR47558+HR0lMV1QbyBPDEhNtRKJUM7dhKN1eDxuFDobKhEg7a9vIvaxkr88zwoDbOTND4+LmvMaP0r+NGxE9iL3DxWU4DO7BBPSUfPxs1iTvMpKrCgs9pRqzVs7nsGb3khxRUurE73Q8HMo4Lnzp9FMpoIBCrpTy1DI0QHtw5gzssjUFXBjk0RFFoL217aispmp3x+NdvXzjwb/f8Lnjp3Bp3XgcNXjMuqx+wtZWD7i7gCPiyuIjxOI2ZPGVt278Fa6MLuEcxhINvm59urN2YFZ5ryj+CZkbNYnAYcJX7UFgcGi53Boa3457ixFRRgdHjRi6xf2LIdt89BTqkfrV1MjUrLN5cnHs3w9CcnUZqz0XrtpKsduHXZbO7vExlbUbtttNY48eiUPDs4iCnXIKrJoWOxD6Pw8Oux/zTlwYMH8uGvP3/l3UP7eOftA3x2/DXeP7CXibEvOX50mCNH9nPp1GE+eHMvkxOX+PjoIRE3zIWTw7y3bze//P6HrDGj9TeZe8vhI+MA8wAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class='gatsby-resp-image-image' alt='Comparison of hyperspectral image segmentation methods' title='' src='/afc-website/static/ce6fc190517636fbeb62231ae57ae8b8/5a190/msiplib.png' srcset='/afc-website/static/ce6fc190517636fbeb62231ae57ae8b8/772e8/msiplib.png 200w,\n/afc-website/static/ce6fc190517636fbeb62231ae57ae8b8/e17e5/msiplib.png 400w,\n/afc-website/static/ce6fc190517636fbeb62231ae57ae8b8/5a190/msiplib.png 800w,\n/afc-website/static/ce6fc190517636fbeb62231ae57ae8b8/c1b63/msiplib.png 1200w,\n/afc-website/static/ce6fc190517636fbeb62231ae57ae8b8/ea964/msiplib.png 1312w' sizes='(max-width: 800px) 100vw, 800px' style='width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;' loading='lazy' decoding='async'>\n  </a>\n    </span>\n<figcaption>Figure 2: Hyperspectral Image Segmentation Comparison.</figcaption>\n</figure>\nComparison of hyperspectral image segmentation methods on the Pavia University dataset. Methods compared include (a) Ground truth, (b) k-means (0.534), (c) GMM (0.517), (d) BGM (0.444), (e) 3D-CAE (0.535), (f) MS-2 (0.564), (g) Ours (0.562).\n<h3 id=\"references\">References</h3>\n<ul>\n<li>\n<p>Cohrs, J.-C., Bajaj, C., &#x26; Berkels, B. (2022). A Distribution-Dependent\nMumford–Shah Model for Unsupervised Hyperspectral Image Segmentation. IEEE\nTransactions on Geoscience and Remote Sensing, 60, 1–21.</p>\n</li>\n<li>\n<p>Song, C., Bajaj, C., &#x26; Huang, Q. (2023). DeblurSR: Event-Based Motion\nDeblurring Under the Spiking Representation. ArXiv Preprint ArXiv:2303.08977.</p>\n</li>\n</ul>","frontmatter":{"title":"Signature Detection & Classification"}}},"pageContext":{"id":"f8215814-08e7-5bf7-9f0b-e1804a4be08e","frontmatter__slug":"/signature-detection","__params":{"frontmatter__slug":"signature-detection"}}},"staticQueryHashes":["429448491"],"slicesMap":{}}