{"componentChunkName":"component---src-pages-markdown-remark-frontmatter-slug-js","path":"/integration/","result":{"data":{"markdownRemark":{"html":"<h1 id=\"integration-of-ireo-video-streams\">Integration of IR/EO Video Streams</h1>\n<p>The integration of Infrared (IR) and Electro-Optical (EO) video streams stands\nas a complex and critical challenge in the realms of computer vision and\nsurveillance technologies. Our contributions in this sector seek to pioneer new\nfrontiers by utilizing innovative techniques that address both traditional and\nemergent challenges in object detection, shape matching, and intensity\nrecovery.</p>\n<p>In the realm of consistent shape matching, our work draws upon the GenCorres\nmethod, offering a robust solution for matching and aligning diverse\ngeometries. Complementing this, the E-CIR technique addresses the continuous\nintensity recovery, adapting novel principles of information retrieval and\nobject recognition.</p>\n<p>Further augmenting our approach is the integration of recent advancements in\nadversarial attack pipelines, as detailed in \"Learning Transferable 3D\nAdversarial Cloaks for Deep Trained Detectors\" (Maesumi et al., 2021). This\nmethod introduces a novel patch-based adversarial attack that trains\nadversarial patches on 3D human meshes, offering an innovative attacking scheme\nwith real-world robustness. Contrary to traditional adversarial attacks, this\napproach incorporates deformation consistent with real-world materials,\nrendering it effective under varying views. Collectively, these groundbreaking\nmethods form the core of our strategy for integrating IR and EO video streams.\nIn the subsequent sections, we elucidate the technicalities of these\ncontributions, emphasizing their relevance to the broader scope of our project.</p>\n<h2 id=\"gencorres-consistent-shape-matching-via-coupled-implicit-explicit-shape-generative-models\">GenCorres: Consistent Shape Matching via Coupled Implicit-Explicit Shape Generative Models</h2>\n<p>GenCorres introduces a groundbreaking unsupervised joint shape matching (JSM)\napproach (Yang et al., 2023). At its core, GenCorres learns a parametric\nmesh generator to fit deformable shapes, preserving local geometric structures\nand enforcing consistent correspondences. Three primary advantages define\nGenCorres:</p>\n<ol>\n<li>\n<p><strong>Data-Driven Power of JSM:</strong> GenCorres performs JSM on a synthetic\nshape collection much larger than the input shapes, maximizing the\ndata-driven potential.</p>\n</li>\n<li>\n<p><strong>Unified Matching Approach:</strong> GenCorres combines consistent shape\nmatching and pairwise matching by enforcing deformation priors\nbetween adjacent synthetic shapes.</p>\n</li>\n<li>\n<p><strong>Concise Encoding:</strong> The generator within GenCorres encapsulates a\nsimplified encoding of consistent shape correspondences.</p>\n</li>\n</ol>\n<p>Despite the inherent challenges in learning a mesh generator from unorganized\nshapes, GenCorres overcomes local minima by utilizing an implicit generator to\nprovide intermediate shapes. Experimental results attest to the substantial\nsuperiority of GenCorres over existing JSM techniques, with synthetic shapes\npreserving local geometric features and outperforming competitive deformable\nshape generators.</p>\n<h2 id=\"e-cir-event-enhanced-continuous-intensity-recovery\">E-CIR: Event-Enhanced Continuous Intensity Recovery</h2>\n<p>E-CIR contributes a novel approach to the conversion of blurry images into\nsharp videos (Song et al., 2022). Motion blur, a pervasive visual artifact, is\nresolved by E-CIR through the parametric representation of time-to-intensity\nfunctions. By leveraging events as auxiliary input, E-CIR constructs parametric\nbases and trains a deep learning model to predict function coefficients. The\nintroduction of a refinement module further ensures consistency across\nconsecutive frames.</p>\n<p>Comparatively, E-CIR delivers smoother and more realistic results against\nexisting event-enhanced deblurring methods. The approach bridges the\nintegration of IR/EO video streams, providing a robust solution to motion blur\nand intensity recovery.</p>\n<p>Figure 1 offers a comprehensive view of E-CIR's structure, detailing the\nstep-by-step flow from blurry image input to sharp video output, utilizing the\ninnovations of event-enhanced processing and refinement techniques.</p>\n<figure id='e-cir'>\n<span class='gatsby-resp-image-wrapper' style='position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; '>\n      <a class='gatsby-resp-image-link' href='/static/5ade946eb53caa768a569505ad9df958/dcb79/E-CIR.png' style='display: block' target='_blank' rel='noopener'>\n    <span class='gatsby-resp-image-background-image' style=\"padding-bottom: 42.5%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAACaElEQVR42iWQ2y/bcRjG+49MCInIiohjiOOFCxcuuBBBXYihwdhE7ELcCfMHsCwRi4uZy2WabMvi0NKqlla11d+hVXVqqba0OtvFZ7/+dvHk+7yn53nfr8ZzZka6tCKGrAiXDqQLC9KVXcn9h3hlQ7y2EYg4CIQOkEQTsYcLkrd2UikXT89ekukM3KTSZ2isjp+IwS3uY1+JRH9wd7+BdHOAP6JAeQN+C37JjP/aihy2Kmb73D8GiT94iMQ8xJNu0n9OSP91kX6W0didm8pWJlKJz8RvPhKLf8Mlmzhyb3Iq73Jm3CG0s4Pg3cXi/I7Dt81dTObE52Zr/xQh4OYmbCYUMvLwKCqCnl8EwiYSicyGBqKxDWUTG75g5issyLIZ36lRucKCV97D698jGg+QSHoJR90kUgrih8SjNpIZQTFwxLHPiEswKWd5kIL7SmxCOLcTjJxgO97GerjJ+a0bIXSIU9hVjIMkn0QurqzcRp08/fYrJi4idz40CwvveTUwwOLiB9a/rDM+/oaJiQnW1taYnp6mt1fH0uISq6urjI2NMTX1DoPBwMzMDENDelZWPrG8vMzo6Gvm5ubRtLS0MDg4yOTkJJ2dnXR1dTEyMkJ/fz/Nzc309PSocXt7O93d3SofHh6mtbUVvV6vmL+lra0NnU7H7OwsmqwXWRQWFlJSUkJubi4FBQWUl5eTl5dHdna2WistLVV5plZZWUl+fr7aW1ZWRnFxscpzcnLo6+tDU1FRgVarVQXq6+tpaGigqKhIFW1qaqK6upq6ujo139jYSE1NDVVVVdTW1qp5rfalOpfp7+jo4B83LAM+bMLjLAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class='gatsby-resp-image-image' alt='The architecture of E-CIR showing the process of converting a blurry image into a sharp video. The diagram illustrates how events are leveraged to construct parametric bases, the deep learning model for predicting coefficients, and the refinement module to ensure visual consistency across frames.' title='' src='/static/5ade946eb53caa768a569505ad9df958/5a190/E-CIR.png' srcset='/static/5ade946eb53caa768a569505ad9df958/772e8/E-CIR.png 200w,\n/static/5ade946eb53caa768a569505ad9df958/e17e5/E-CIR.png 400w,\n/static/5ade946eb53caa768a569505ad9df958/5a190/E-CIR.png 800w,\n/static/5ade946eb53caa768a569505ad9df958/c1b63/E-CIR.png 1200w,\n/static/5ade946eb53caa768a569505ad9df958/29007/E-CIR.png 1600w,\n/static/5ade946eb53caa768a569505ad9df958/dcb79/E-CIR.png 1700w' sizes='(max-width: 800px) 100vw, 800px' style='width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;' loading='lazy' decoding='async'>\n  </a>\n    </span>\n<figcaption>Figure 1: E-CIR Architecture</figcaption>\n</figure>\n<p><strong>Code:</strong>\nThe code for E-CIR: Event-Enhanced Continuous Intensity Recovery is available\nin SharePoint. This PyTorch-based implementation comprises two main modules:\nthe initialization module, responsible for regressing polynomial coefficients\nfrom the events and the blurry frame, and the refinement module, which polishes\nframe quality through visual feature propagation. For experimentation, E-CIR\nwas tested on the REDS dataset and seven real event captures in EDI. The REDS\ndataset is a REalistic and Dynamic Scenes (REDS) dataset for video deblurring\nand super-resolution. Detailed instructions on setting up these datasets can be\nfound in the corresponding REDS_Dataset.md and EDI_Dataset.md files.</p>\n<h2 id=\"learning-transferable-3d-adversarial-cloaks-for-deep-trained-detectors\">Learning Transferable 3D Adversarial Cloaks for Deep Trained Detectors</h2>\n<figure id='3DCloakArchitecture'>\n<span class='gatsby-resp-image-wrapper' style='position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; '>\n      <a class='gatsby-resp-image-link' href='/static/b57574fc736fb9b2107809b171dd6314/d4b10/3DCloakArchitecture.png' style='display: block' target='_blank' rel='noopener'>\n    <span class='gatsby-resp-image-background-image' style=\"padding-bottom: 47.5%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAACSElEQVR42l1SW4vTUBDOf/E3CCL+DX1QEFwKoixUHxRZ0SddhAr6JIgPPnij2mIf3AfBKisFlbbbtZf0sps2bdOkm20ubZM0SZN+zjmLq+yBw8yZM8w33zcj4MQJggCapkLXdQRhgCiOoVs+NCPA8MDBzA2Ocy3Lgig2UK3V0RCbiClXcBwHruvirx2NRiiVy9gpV6ANFcymDl6kv+N5+icev6riR0XmxaIogqIouL/5FFeTd/Ew9Qyz2QxCq9VCtVpFv9/nxWzbwpt3aXzM5oC5D0tXsZm6g+vrCWzcW8OnrffoyT3Ich+5bAY3rlzCuTOnkVi/iTkraJom2GXte55H2DGGhKyNx7yTQ2uEa8kLuL2xhvOXT+HzlyxcylM1DblMBk9uJZC8eBaPUg8QLSMIIWm2JMcnGxMN1/UwGAw4HZdkYLFCYRuNxi7yX7cIrE90l1wexupX4RtqxTwmhkF1lhCYmI3dCur1OpSRQpRt1EiCyk4ZhmHyLler1b+pkTudW1ya/0JcU3YEXbfxO5eDMZnAsE1OV1cVdJs1zB3qQpbR6rTgLub0nlHnQxRLRZRKJQ7OThxHx6BCGHl4+fY1bNMlGg6fVFfaR61WRRiG8PwFVKkHh+L21MaEgJl+qqpisVic3DoIUeTjw3Yerr+ER7qwRFEU+WW+dqBhr9WERfQd+leGQ+x39iBJEsa0AVJXQrfbRa/XO1obNmF9cojpdHqETkVYgkxUNeqEWYne7M8g4ceqBqne5Jp3Om2+cu12mw+RSfAHOg/PXkSIOfoAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class='gatsby-resp-image-image' alt='3D adversarial logo pipeline' title='' src='/static/b57574fc736fb9b2107809b171dd6314/5a190/3DCloakArchitecture.png' srcset='/static/b57574fc736fb9b2107809b171dd6314/772e8/3DCloakArchitecture.png 200w,\n/static/b57574fc736fb9b2107809b171dd6314/e17e5/3DCloakArchitecture.png 400w,\n/static/b57574fc736fb9b2107809b171dd6314/5a190/3DCloakArchitecture.png 800w,\n/static/b57574fc736fb9b2107809b171dd6314/c1b63/3DCloakArchitecture.png 1200w,\n/static/b57574fc736fb9b2107809b171dd6314/d4b10/3DCloakArchitecture.png 1394w' sizes='(max-width: 800px) 100vw, 800px' style='width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;' loading='lazy' decoding='async'>\n  </a>\n    </span>\n<figcaption>Figure 2: 3D Adversarial Logo Pipeline.</figcaption>\n</figure>\n<figure id='3DCloakExample'>\n<span class='gatsby-resp-image-wrapper' style='position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; '>\n      <a class='gatsby-resp-image-link' href='/static/1444d77bf19bac83329226b150e083ff/47218/3DCloakExample.png' style='display: block' target='_blank' rel='noopener'>\n    <span class='gatsby-resp-image-background-image' style=\"padding-bottom: 50.5%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAACvklEQVR42i3SXVMaBxTG8f0c6X1z26YZpy8XbWKSVp2BRC2JCmEXUYSFuAQFRcYXsrKwvAmL2ipBR1IlRgFJxiYV69imSeNMZtrMJHe5bvsp/t2LXDxznttzfkcwdluUfz7DaLRZ+/M/jN0DNh6fEXvwBrX2jukfsvgTItmtpySqx8wub/Pw9D1Hz/8irc6jeN1k4tP0dV9DCfgRhkMyjrtB5lbyREo57mpB1PuPkLNbxErrjCy6sYauoC+VyBo65bU486tVSgevCXvHSEd9hP3f8+nH5+i19CB0Wbr45JsO4oaB3W3DKl5CTvTjUixYr3czMDrEJXsnmxtrRCN+FJ+Ia8TBbZ9EzONBHbMT8PQyOXqVaMiFYORiZBI+9HtOwhPdaAmJzWaByqMietZMOsNiJklotpfhQCcuqYt+sYPvbBeZkn2MizaMuVushL7ANXAD4ZfjNRqHKntPVGrNKA8PUjw7bNNq7vHTzn1z1mns7nEnfhP79FesVxcob8wwpdlRo2HmJgJsF0M8K1hYmJQRcjt1lp+8ILf/lOWX/5Kr1amYKNHq3yR23xJZSeNVHeQftNG2TogWtqm/+IfjP96S1eIEfR70+RlumCjB8QCCFPQyZJawuZ6sZU0UhXh5B0+qipIq414cMVGuoucLZEsZlvIaU7kKRvOMiNdDMuzHLw5y4fxHH1Cs3XzW+TkzmTzXb/VgcV42UfqQlH5uO4boc9q57LjC5uaP5o0DSA4bDmnQRHESG/OgySLhO4NMuL8lNjmMsFqcI5+UyWk+otM2kslhqq0S67VV7mkp0ukiqXwaJWbFPvY1Hp8dm/SliXLBfJtRZOcAxVmRZaUD8aYF4eT3CkenGQ7bZo7TZoo8/+0lJ+1TGs0Dfm0f0arXiehOxvUeavtL7DcKqCUZQ9dYWlygVUnyaksy+wz/A0S99IX0cAdgAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class='gatsby-resp-image-image' alt='Examples of our adversarial attack against Faster R-CNN. The first row contains human meshes without any adversarial perturbation; Faster R-CNN is 99% confident in its human predictions in these images. The second row displays the cloaking effect of an adversarial patch trained by the pipeline outlined in Figure. To bolster our attack robustness, we train and test our adversarial logos on meshes with a diverse set of surface-level and body-level deformations. The figure above features running, walking, and idle poses on meshes of various shapes and sizes, which are sampled from the SURREAL dataset. We even observe attack success for partially occluded adversarial textures (e.g. the third column).' title='' src='/static/1444d77bf19bac83329226b150e083ff/5a190/3DCloakExample.png' srcset='/static/1444d77bf19bac83329226b150e083ff/772e8/3DCloakExample.png 200w,\n/static/1444d77bf19bac83329226b150e083ff/e17e5/3DCloakExample.png 400w,\n/static/1444d77bf19bac83329226b150e083ff/5a190/3DCloakExample.png 800w,\n/static/1444d77bf19bac83329226b150e083ff/c1b63/3DCloakExample.png 1200w,\n/static/1444d77bf19bac83329226b150e083ff/47218/3DCloakExample.png 1344w' sizes='(max-width: 800px) 100vw, 800px' style='width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;' loading='lazy' decoding='async'>\n  </a>\n    </span>\n<figcaption>Figure 3: Adversarial Attack Examples against Faster R-CNN.</figcaption>\n</figure>\n<p>The advent of \"Learning Transferable 3D Adversarial Cloaks for Deep Trained\nDetectors\" (Maesumi et al., 2021) marks a transformative moment in the\nworld of adversarial research. This method presents a groundbreaking\nadversarial attack pipeline that crafts adversarial patches on 3D human meshes.\nThe architecture is shown in Figure 1, while\nexample results are shown in Figure 2. Key contributions\nand characteristics of this approach are summarized below:</p>\n<ol>\n<li>\n<p><strong>Innovative Patch-Based Attack:</strong> Unlike traditional adversarial\nattacks that append patches, this new form of attack maps into the\n3D object world and is back-propagated through differentiable\nrendering. This creates adversarial textures on 3D human meshes,\nleading to an attacking scheme that maintains its efficacy in the\nphysical world.</p>\n</li>\n<li>\n<p><strong>Real-World Robustness:</strong> The adversarial patches are trained under\ndeformations consistent with real-world materials, showcasing the\nability to fool state-of-the-art deep object detectors even under\nvarying views. This represents a significant advance over existing\nmethods.</p>\n</li>\n<li>\n<p><strong>Transferable Adversarial Design:</strong> The created 3D adversarial\npatches can be transferred to human meshes in various poses and\nrendered onto real-world background images. This contributes to an\nenhanced and more versatile attacking scheme.</p>\n</li>\n<li>\n<p><strong>Potential Implications:</strong> The proposed method's persistent\nstrength in physical-world scenarios offers a rich avenue for\nexploration in both adversarial defense mechanisms and real-world\nobject detection strategies.</p>\n</li>\n</ol>\n<p>In the context of the integration of IR/EO video streams, this innovative\nmethod introduces a complex layer of considerations related to object detection\nand adversarial resilience. Through the synthesis of 3D object modeling and\nadversarial texture creation, this method contributes to an evolving landscape\nof techniques that must be handled with care in the deployment of IR/EO\nsystems. Experimental results further emphasize the effectiveness and\nuniqueness of this approach, suggesting promising directions for future\nresearch and applications.</p>\n<p><strong>Code:</strong>\nThe code is available in SharePoint.</p>\n<h4 id=\"references\">References</h4>\n<ul>\n<li>\n<p>Maesumi, A., Zhu, M., Wang, Y., Chen, T., Wang, Z., &#x26; Bajaj, C. (2021).\nLearning transferable 3D adversarial cloaks for deep trained detectors. ArXiv\nPreprint ArXiv:2104.11101.</p>\n</li>\n<li>\n<p>Yang, H., Huang, X., Sun, B., Bajaj, C., &#x26; Huang, Q. (2023). GenCorres:\nConsistent Shape Matching via Coupled Implicit-Explicit Shape Generative\nModels. ArXiv Preprint ArXiv:2304.10523.</p>\n</li>\n<li>\n<p>Song, C., Huang, Q., &#x26; Bajaj, C. (2022). E-cir: Event-enhanced continuous\nintensity recovery. Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 7803â€“7812.</p>\n</li>\n</ul>","frontmatter":{"title":"Integration of IR/EO Video Streams"}}},"pageContext":{"id":"0323f746-c828-503e-b0e5-c86ff8a33cc0","frontmatter__slug":"/integration","__params":{"frontmatter__slug":"integration"}}},"staticQueryHashes":["429448491"],"slicesMap":{}}